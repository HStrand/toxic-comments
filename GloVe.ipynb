{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_DIMS = 300\n",
    "pretrained = 'data\\\\glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(row):\n",
    "    row = row.strip().split()\n",
    "    # can't use row[0], row[1:] split because 840B contains multi-part words \n",
    "    word, arr = \" \".join(row[:-N_DIMS]), row[-N_DIMS:]\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove():\n",
    "    return dict(get_coefs(row) for row in open(pretrained, encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(row):\n",
    "    return re_tok.sub(r' \\1 ', row).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "def lengths(series):\n",
    "    return np.array(series.apply(len)).reshape(-1,1).astype(float)\n",
    "\n",
    "def asterixes(series):\n",
    "    return np.array(series.apply(lambda x: x.count('!'))).reshape(-1,1).astype(float)\n",
    "\n",
    "def uppercase_count(series):\n",
    "    return np.array(series.apply(lambda x: len(re.findall(r'[A-Z]',x)))).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_wordvector(tokens_list, vector, generate_missing=False, k=N_DIMS):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments.apply(lambda x: get_average_wordvector(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def embed(series):\n",
    "    return get_embeddings(glove, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp_pipeline import NlpPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv')\n",
    "test = pd.read_csv('data\\\\test.csv')\n",
    "train[\"comment_text\"] = train[\"comment_text\"].fillna(\"_na_\")\n",
    "test[\"comment_text\"] = test[\"comment_text\"].fillna(\"_na_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%time glove = get_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [column for column in train.columns[2:8]]\n",
    "feature_funcs = [lengths, asterixes, uppercase_count]\n",
    "transforms = [tokenize]\n",
    "# logreg = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg')\n",
    "# logreg.name = \"Logistic regression newton\"\n",
    "xgb = XGBClassifier(n_estimators=300, max_depth=5, learning_rate=0.15, colsample_bytree=0.6)\n",
    "xgb.name = \"XGBoost\"\n",
    "models = [xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = NlpPipeline(train, test, \"comment_text\", class_labels, feature_funcs, transforms, models, word_vectors=glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features\n"
     ]
    }
   ],
   "source": [
    "pipe.engineer_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms\n"
     ]
    }
   ],
   "source": [
    "pipe.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%time pipe.create_embeddings(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating\n",
      "LogisticRegression(C=30.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "Cross-validating toxic\n",
      "roc_auc: 0.9663566879765998\n",
      "Cross-validating severe_toxic\n",
      "roc_auc: 0.9824378952854586\n",
      "Cross-validating obscene\n",
      "roc_auc: 0.975907595417055\n",
      "Cross-validating threat\n",
      "roc_auc: 0.9669333937159189\n",
      "Cross-validating insult\n",
      "roc_auc: 0.9721503138305995\n",
      "Cross-validating identity_hate\n",
      "roc_auc: 0.966883488850247\n"
     ]
    }
   ],
   "source": [
    "pipe.cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and predicting\n",
      "Fitting submission classifier for toxic\n",
      "Fitting submission classifier for severe_toxic\n",
      "Fitting submission classifier for obscene\n",
      "Fitting submission classifier for threat\n",
      "Fitting submission classifier for insult\n",
      "Fitting submission classifier for identity_hate\n"
     ]
    }
   ],
   "source": [
    "pipe.fit_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "pipe.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.models = [xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating\n",
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.25, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "Cross-validating toxic\n",
      "roc_auc: 0.9370200898161059\n",
      "Cross-validating severe_toxic\n",
      "roc_auc: 0.9710925528863775\n",
      "Cross-validating obscene\n",
      "roc_auc: 0.9501719761273542\n",
      "Cross-validating threat\n",
      "roc_auc: 0.9528030564046871\n",
      "Cross-validating insult\n",
      "roc_auc: 0.9514289069299136\n",
      "Cross-validating identity_hate\n",
      "roc_auc: 0.9435259598984205\n"
     ]
    }
   ],
   "source": [
    "pipe.cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predictions['XGBoost'] = pipe.predictions['Logistic regression newton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "pipe.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-71234dea8bf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mscorelist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pipe.train_features, list(pipe.train[\"severe_toxic\"]), test_size=0.2, random_state=40)\n",
    "xgb.fit(X_train, y_train, eval_metric='auc')\n",
    "y_pred = xgb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877136940558803"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating toxic\n",
      "roc_auc: 0.967015805216979\n",
      "Cross-validating severe_toxic\n",
      "roc_auc: 0.9853333628435932\n",
      "Cross-validating obscene\n",
      "roc_auc: 0.9778539295697459\n",
      "Cross-validating threat\n",
      "roc_auc: 0.9820443915227862\n",
      "Cross-validating insult\n",
      "roc_auc: 0.9739457957004263\n",
      "Cross-validating identity_hate\n",
      "roc_auc: 0.9730230143579437\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3cb4e9c8cc1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\": \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mscorelist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "scorelist = []\n",
    "for label in pipe.class_labels:\n",
    "    pipe.log(\"Cross-validating \" + label)\n",
    "    scores = cross_val_score(xgb, pipe.train_features, list(pipe.train[label]), scoring=pipe.metric, cv=5)\n",
    "    pipe.log(pipe.metric + \": \" + str(np.mean(scores)))\n",
    "    scorelist.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.cv_scores[\"XGBoost\"] = np.mean(scorelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976536049868579"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.cv_scores[\"XGBoost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting submission classifier for toxic\n"
     ]
    }
   ],
   "source": [
    "for label in pipe.class_labels:\n",
    "    pipe.log(\"Fitting submission classifier for \" + label)\n",
    "    y_train = np.array(pipe.train[label])\n",
    "    xgb.fit(pipe.train_features, y_train)\n",
    "    pipe.predictions[\"XGBoost\"][label] = xgb.predict_proba(pipe.test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.log(\"Creating submissions\")\n",
    "submission = pipe.test[pipe.id_column].to_frame()\n",
    "for label in pipe.class_labels:\n",
    "    submission[label] = pipe.predictions[\"XGBoost\"][label][:,1]\n",
    "\n",
    "submission_num = 1\n",
    "past_submissions = pipe.get_past_submissions()\n",
    "if past_submissions is not None and past_submissions != []:\n",
    "    submission_num = max(past_submissions)[0] + 1\n",
    "filename = 'submissions\\\\submission' + str(submission_num) + '.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "pipe.store_submission_metadata(filename, submission_num, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

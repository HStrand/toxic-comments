{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_DIMS = 100\n",
    "pretrained = 'data\\\\glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(row):\n",
    "    row = row.strip().split()\n",
    "    # can't use row[0], row[1:] split because 840B contains multi-part words \n",
    "    word, arr = \" \".join(row[:-N_DIMS]), row[-N_DIMS:]\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove():\n",
    "    return dict(get_coefs(row) for row in open(pretrained, encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(row):\n",
    "    return re_tok.sub(r' \\1 ', row).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lengths(series):\n",
    "    return np.array(series.apply(len)).reshape(-1,1).astype(float)\n",
    "\n",
    "def asterixes(series):\n",
    "    return np.array(series.apply(lambda x: x.count('!'))).reshape(-1,1).astype(float)\n",
    "\n",
    "def uppercase_count(series):\n",
    "    return np.array(series.apply(lambda x: len(re.findall(r'[A-Z]',x)))).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_wordvector(tokens_list, vector, generate_missing=False, k=N_DIMS):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments.apply(lambda x: get_average_wordvector(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def embed(series):\n",
    "    return get_embeddings(glove, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_pipeline import NlpPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv')\n",
    "test = pd.read_csv('data\\\\test.csv')\n",
    "train[\"comment_text\"] = train[\"comment_text\"].fillna(\"_na_\")\n",
    "test[\"comment_text\"] = test[\"comment_text\"].fillna(\"_na_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = get_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [column for column in train.columns[2:8]]\n",
    "feature_funcs = [lengths, asterixes, uppercase_count]\n",
    "transforms = [tokenize]\n",
    "logreg = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg')\n",
    "logreg.name = \"Logistic regression newton\"\n",
    "models = [logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = NlpPipeline(train, test, \"comment_text\", class_labels, feature_funcs, transforms, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features\n"
     ]
    }
   ],
   "source": [
    "pipe.engineer_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms\n"
     ]
    }
   ],
   "source": [
    "pipe.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n"
     ]
    }
   ],
   "source": [
    "pipe.create_embeddings(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating\n",
      "LogisticRegression(C=30.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "Cross-validating toxic\n"
     ]
    }
   ],
   "source": [
    "pipe.log(\"Cross-validating\") \n",
    "for model in pipe.models:\n",
    "    pipe.log(str(model)) \n",
    "    scorelist = [] \n",
    "    for label in pipe.class_labels:\n",
    "        pipe.log(\"Cross-validating \" + label)\n",
    "        scores = cross_val_score(model, pipe.train_features, list(pipe.train[label]), scoring=pipe.metric, cv=5)\n",
    "        pipe.log(pipe.metric + \": \" + str(np.mean(scores)))\n",
    "        scorelist.append(np.mean(scores))\n",
    "    pipe.cv_scores[model.name] = np.mean(scorelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
    "    embeddings = clean_comments.apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "pretrained = \"data\\\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "def w2v(series):\n",
    "    word_vectors = gensim.models.KeyedVectors.load_word2vec_format(pretrained, binary=True)        \n",
    "    return get_word2vec_embeddings(word_vectors, series)    \n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(row):\n",
    "    return re_tok.sub(r' \\1 ', row).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NlpPipeline():\n",
    "\n",
    "    def __init__(self, train=None, test=None, input_column='text_comment', class_labels=None, feature_functions=None, transforms=None, models=None, metric='roc_auc', id_column='id', verbosity=1):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.input_column = input_column\n",
    "        self.class_labels = class_labels        \n",
    "        self.feature_functions = feature_functions\n",
    "        self.transforms = transforms\n",
    "        self.models = models\n",
    "        self.metric = metric\n",
    "        self.id_column = id_column\n",
    "        self.verbosity = verbosity\n",
    "        self.train_features = np.array([])\n",
    "        self.test_features = np.array([])\n",
    "        self.cv_scores = {}\n",
    "        for model in self.models:\n",
    "            self.cv_scores[model.name] = -1\n",
    "        self.scaler = StandardScaler()\n",
    "        self.train_transformed = self.train[input_column]\n",
    "        self.test_transformed = self.test[input_column]\n",
    "\n",
    "    def run(self):        \n",
    "        self.engineer_features()\n",
    "        self.apply_transforms()\n",
    "        self.create_embeddings(w2v)\n",
    "        self.train_test()\n",
    "        # self.cross_val()\n",
    "        self.fit_predict()\n",
    "        self.create_submission()\n",
    "\n",
    "    def log(self, s):\n",
    "        if self.verbosity > 0:\n",
    "            print(s)\n",
    "            \n",
    "    def create_embeddings(self, func):\n",
    "        self.log(\"Creating embeddings\")\n",
    "        embeddings = func(self.train_transformed)\n",
    "        \n",
    "        self.train_features = np.hstack((self.train_features, np.array(embeddings[:len(train[input_column])])))\n",
    "        self.test_features = np.hstack((self.test_features, np.array(embeddings[len(train[input_column]):])))\n",
    "\n",
    "    def engineer_features(self, use_transform=False, normalize=True):\n",
    "        self.log(\"Engineering features\")\n",
    "        train_feats = []\n",
    "        test_feats = []\n",
    "        if use_transform:\n",
    "            train_data = self.train_transformed\n",
    "            test_data = self.test_transformed\n",
    "        else:\n",
    "            train_data = self.train[input_column]\n",
    "            test_data = self.test[input_column]\n",
    "            \n",
    "        for func in self.feature_functions:\n",
    "            train_feature = np.array(func(train_data))            \n",
    "            test_feature = np.array(func(test_data))            \n",
    "            if normalize:\n",
    "                train_feature = self.normalize(train_feature)\n",
    "                test_feature = self.normalize(test_feature)\n",
    "            train_feats.append(train_feature)\n",
    "            test_feats.append(test_feature)            \n",
    "\n",
    "        self.train_features = np.hstack((feature for feature in train_feats))\n",
    "        self.test_features = np.hstack((feature for feature in test_feats))\n",
    "        \n",
    "    def add_feature(self, func, use_transform=False, normalize=True):\n",
    "        self.log(\"Adding feature\")\n",
    "        self.feature_functions.append(func)\n",
    "        if use_transform:\n",
    "            train_data = self.train_transformed\n",
    "            test_data = self.test_transformed\n",
    "        else:\n",
    "            train_data = self.train[input_column]\n",
    "            test_data = self.test[input_column]\n",
    "        \n",
    "        train_feature = np.array(func(train_data))\n",
    "        test_feature = np.array(func(test_data))\n",
    "        if normalize:\n",
    "            train_feature = self.normalize(train_feature)\n",
    "            test_feature = self.normalize(test_feature)\n",
    "        \n",
    "        self.train_features = np.hstack((self.train_features, np.array(train_feature)))\n",
    "        self.test_features = np.hstack((self.test_features, np.array(test_feature)))\n",
    "        \n",
    "    def normalize(self, data):\n",
    "        self.scaler.fit(data)\n",
    "        return self.scaler.transform(data)\n",
    "    \n",
    "    def apply_transforms(self):        \n",
    "        self.log(\"Applying transforms\")\n",
    "        for transform in self.transforms:\n",
    "            self.train_transformed = self.train[self.input_column].apply(transform)\n",
    "            self.test_transformed = self.test[self.input_column].apply(transform)\n",
    "\n",
    "    def train_test(self):\n",
    "        self.log(\"Training and testing\") \n",
    "        for model in self.models:\n",
    "            self.log(str(model)) \n",
    "            scorelist = [] \n",
    "            for label in self.class_labels:\n",
    "                self.log(\"Fitting classifier for \" + label)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(self.train_features, list(self.train[label]), test_size=0.2, random_state=40)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict_proba(X_test)\n",
    "                self.log(self.metric + str(roc_auc_score(y_test, y_pred[:,1])))\n",
    "                scorelist.append(np.mean(roc_auc_score(y_test, y_pred[:,1])))\n",
    "            self.cv_scores[model.name] = np.mean(scorelist)\n",
    "\n",
    "    def cross_val(self):\n",
    "        self.log(\"Cross-validating\") \n",
    "        for model in self.models:\n",
    "            self.log(str(model)) \n",
    "            scorelist = [] \n",
    "            for label in self.class_labels:\n",
    "                self.log(\"Cross-validating \" + label)\n",
    "                scores = cross_val_score(model, self.train_features, list(train[label]), scoring=self.metric, cv=5)\n",
    "                self.log(self.metric + str(np.mean(scores)))\n",
    "                scorelist.append(np.mean(scores))\n",
    "            self.cv_scores[model.name] = np.mean(scorelist)\n",
    "\n",
    "    def fit_predict(self):\n",
    "        self.log(\"Fitting and predicting\") \n",
    "        self.predictions = {}\n",
    "        for model in self.models:\n",
    "            self.predictions[model.name] = {}\n",
    "            for label in self.class_labels:\n",
    "                self.log(\"Fitting submission classifier for \" + label)\n",
    "                y_train = np.array(train[label])\n",
    "                model.fit(self.train_features, y_train)\n",
    "                self.predictions[model.name][label] = model.predict_proba(self.test_features)\n",
    "\n",
    "    def create_submission(self):\n",
    "        for model in self.models:\n",
    "            self.log(\"Creating submissions\")\n",
    "            submission = self.test[self.id_column].to_frame()\n",
    "            for label in self.class_labels:\n",
    "                submission[label] = self.predictions[model.name][label][:,1]\n",
    "            \n",
    "            submission_num = 1\n",
    "            past_submissions = self.get_past_submissions()\n",
    "            if past_submissions is not None and past_submissions != []:\n",
    "                submission_num = max(past_submissions)[0] + 1\n",
    "            filename = 'submissions\\\\submission' + str(submission_num) + '.csv'\n",
    "            submission.to_csv(filename, index=False)\n",
    "            self.store_submission_metadata(filename, submission_num, model)\n",
    "\n",
    "    def get_past_submissions(self):\n",
    "        current_dir = os.getcwd()\n",
    "        path = os.path.join(current_dir, 'submissions')\n",
    "        try:\n",
    "            return [[int(s) for s in re.findall(r'\\d+', f)] for f in os.listdir(path)]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def store_submission_metadata(self, filename, submission_num, model):\n",
    "        feature_funcs = \"\"\n",
    "        transforms = \"\"\n",
    "        for func in self.feature_functions:\n",
    "            feature_funcs += str(func).split(' ')[1] + \" \"\n",
    "        for trf in self.transforms:\n",
    "            transforms += str(trf).split(' ')[1] + \" \"   \n",
    "        cols = [\"submission\", \"filename\", \"model\", \"feature_funcs\", \"transforms\", \"cv_score\"]\n",
    "        metadata = pd.DataFrame([[submission_num, filename, self.model_info(model), feature_funcs, transforms, self.cv_scores[model.name]]], columns=cols)\n",
    "        filename = 'submissions\\\\submeta.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            metadata.to_csv(filename, mode='a', header=False, index=False)\n",
    "        except:            \n",
    "            metadata.to_csv(filename, mode='a', index=False)\n",
    "            \n",
    "    def model_info(self, model):\n",
    "        s = model.name + \":\"\n",
    "        for param in model.get_params():            \n",
    "            s += \" \"\n",
    "            s += str(model.get_params()[param])\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"Train: \"\n",
    "        s += str(self.train.shape)\n",
    "        s += \"\\n\"\n",
    "        s += \"Test: \"\n",
    "        s += str(self.test.shape)\n",
    "        s += \"\\n\"\n",
    "        s += \"Train features: \"\n",
    "        s += str(self.train_features.shape)\n",
    "        s += \"\\n\"\n",
    "        s += \"Test features: \"\n",
    "        s += str(self.test_features.shape)\n",
    "        s += \"\\n\"\n",
    "        s += \"Input column: \"\n",
    "        s += self.input_column\n",
    "        s += \"\\n\"\n",
    "        s += \"Class labels:\"\n",
    "        for label in self.class_labels:\n",
    "            s += \" \"\n",
    "            s += label\n",
    "        s += \"\\n\"\n",
    "        s += \"Models: \"\n",
    "        for model in self.models:            \n",
    "            s += self.model_info(model)\n",
    "            s += \" | \"\n",
    "            \n",
    "        s += \"\\n\"\n",
    "        s += \"Transforms: \"\n",
    "        for transform in self.transforms:\n",
    "            s += \" \"\n",
    "            s += str(transform).split(' ')[1]\n",
    "        s += \"\\n\"\n",
    "        s += \"Feature functions: \"\n",
    "        for func in self.feature_functions:\n",
    "            s += \" \"\n",
    "            s += str(func).split(' ')[1]\n",
    "        s += \"\\n\"\n",
    "        s += \"Metric: \"\n",
    "        s += self.metric\n",
    "        s += \"\\n\"\n",
    "        s += \"CV scores: \"\n",
    "        s += str(self.cv_scores)\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv')\n",
    "test = pd.read_csv('data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column = 'comment_text'\n",
    "class_labels = [column for column in train.columns[2:8]]\n",
    "feature_funcs = [lengths, asterixes, uppercase_count]\n",
    "transforms = [tokenize]\n",
    "logreg = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg')\n",
    "logreg.name = \"Logistic regression newton\"\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.name = \"Logistic regression linear\"\n",
    "models = [logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = NlpPipeline(train, test, input_column, class_labels, feature_funcs, transforms, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms\n"
     ]
    }
   ],
   "source": [
    "pipeline.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features\n"
     ]
    }
   ],
   "source": [
    "pipeline.engineer_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lengths(series):\n",
    "    return np.array(series.apply(len)).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature\n"
     ]
    }
   ],
   "source": [
    "pipeline.add_feature(lengths, use_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train: (159571, 8)\n",
       "Test: (153164, 2)\n",
       "Train features: (159571, 301)\n",
       "Test features: (153164, 301)\n",
       "Input column: comment_text\n",
       "Class labels: toxic severe_toxic obscene threat insult identity_hate\n",
       "Models: Logistic regression newton: 30.0 balanced False True 1 100 ovr 1 l2 None newton-cg 0.0001 0 False | \n",
       "Transforms:  tokenize\n",
       "Feature functions:  w2v\n",
       "Metric: roc_auc\n",
       "CV scores: {'Logistic regression C=30 balanced newton-cg': 0.96642205385485747, 'Logistic regression C=1': 0.96450135056771169, 'Logistic regression newton': 0.96642205385485747, 'Logistic regression linear': 0.96450135056771169}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=30.0, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating\n",
      "LogisticRegression(C=30.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "Cross-validating toxic\n",
      "roc_auc0.957181539826\n",
      "Cross-validating severe_toxic\n",
      "roc_auc0.979454310046\n",
      "Cross-validating obscene\n",
      "roc_auc0.966984833341\n",
      "Cross-validating threat\n",
      "roc_auc0.967254666722\n",
      "Cross-validating insult\n",
      "roc_auc0.964695071303\n",
      "Cross-validating identity_hate\n",
      "roc_auc0.959227665704\n"
     ]
    }
   ],
   "source": [
    "pipeline4.cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train: (159571, 8)\n",
       "Test: (153164, 2)\n",
       "Train features: (159571, 302)\n",
       "Test features: (153164, 302)\n",
       "Input column: comment_text\n",
       "Class labels: toxic severe_toxic obscene threat insult identity_hate\n",
       "Models: Logistic regression newton: 30.0 balanced False True 1 100 ovr 1 l2 None newton-cg 0.0001 0 False | \n",
       "Transforms:  tokenize\n",
       "Feature functions:  w2v lengths asterixes\n",
       "Metric: roc_auc\n",
       "CV scores: {'Logistic regression C=30 balanced newton-cg': 0.96642205385485747, 'Logistic regression C=1': 0.96450135056771169, 'Logistic regression newton': 0.96579968115716197, 'Logistic regression linear': 0.96450135056771169}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lengths(series):\n",
    "    return np.array(series.apply(len)).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asterixes(series):\n",
    "    return np.array(series.apply(lambda x: x.count('!'))).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uppercase_count(series):\n",
    "    return np.array(series.apply(lambda x: len(re.findall(r'[A-Z]',x)))).reshape(-1,1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature\n"
     ]
    }
   ],
   "source": [
    "pipeline4.add_feature(uppercase_count)\n",
    "pipeline4.feature_functions.append(uppercase_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating\n",
      "LogisticRegression(C=30.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "Cross-validating toxic\n",
      "roc_auc0.959831859074\n",
      "Cross-validating severe_toxic\n",
      "roc_auc0.98111790585\n",
      "Cross-validating obscene\n",
      "roc_auc0.968355363638\n",
      "Cross-validating threat\n",
      "roc_auc0.967321720965\n",
      "Cross-validating insult\n",
      "roc_auc0.965616720284\n",
      "Cross-validating identity_hate\n",
      "roc_auc0.959071625642\n"
     ]
    }
   ],
   "source": [
    "pipeline4.cross_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "pipeline4.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic regression C=1': 0.96450135056771169,\n",
       " 'Logistic regression C=30 balanced newton-cg': 0.96642205385485747,\n",
       " 'Logistic regression linear': 0.96450135056771169,\n",
       " 'Logistic regression newton': 0.96688586590880032}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p5 = NlpPipeline(train, test, input_column, class_labels, feature_funcs, transforms, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5.models = pipeline4.models\n",
    "p5.predictions = pipeline4.predictions\n",
    "p5.train_features = pipeline4.train_features\n",
    "p5.test_features = pipeline4.test_features\n",
    "p5.feature_functions = pipeline4.feature_functions\n",
    "p5.cv_scores = pipeline4.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train: (159571, 8)\n",
       "Test: (153164, 2)\n",
       "Train features: (159571, 303)\n",
       "Test features: (153164, 303)\n",
       "Input column: comment_text\n",
       "Class labels: toxic severe_toxic obscene threat insult identity_hate\n",
       "Models: Logistic regression newton: 30.0 balanced False True 1 100 ovr 1 l2 None newton-cg 0.0001 0 False | \n",
       "Transforms:  tokenize\n",
       "Feature functions:  w2v lengths asterixes uppercase_count\n",
       "Metric: roc_auc\n",
       "CV scores: {'Logistic regression C=30 balanced newton-cg': 0.96642205385485747, 'Logistic regression C=1': 0.96450135056771169, 'Logistic regression newton': 0.96688586590880032, 'Logistic regression linear': 0.96450135056771169}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "p5.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p6 = NlpPipeline(train, test, input_column, class_labels, feature_funcs, transforms, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features\n"
     ]
    }
   ],
   "source": [
    "p6.engineer_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transforms\n"
     ]
    }
   ],
   "source": [
    "p6.apply_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-483-605201f67877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-477-3ec45e4887a6>\u001b[0m in \u001b[0;36mcreate_embeddings\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating embeddings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-8789597ef667>\u001b[0m in \u001b[0;36mw2v\u001b[1;34m(series)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_word2vec_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p6.create_embeddings(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.w2v>"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

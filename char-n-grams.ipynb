{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv').fillna(' ')\n",
    "test = pd.read_csv('data\\\\test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 6),\n",
    "    stop_words='english',\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_features.tocsr()\n",
    "test_features = test_features.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 60000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def get_mdl(x, y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.zeros((len(test), len(class_names)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.read_csv('data\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.zeros((len(train), len(class_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "0.962930469030776\n",
      "fit toxic\n",
      "0.9618735741865077\n",
      "fit toxic\n",
      "0.9639397137864535\n",
      "fit toxic\n",
      "0.9662076463464705\n",
      "fit toxic\n",
      "0.9632339798265157\n",
      "fit severe_toxic\n",
      "0.9822216726745506\n",
      "fit severe_toxic\n",
      "0.9849048617270015\n",
      "fit severe_toxic\n",
      "0.986091274828139\n",
      "fit severe_toxic\n",
      "0.9824602761740998\n",
      "fit severe_toxic\n",
      "0.9814096213444636\n",
      "fit obscene\n",
      "0.9854751993512636\n",
      "fit obscene\n",
      "0.9783850420512978\n",
      "fit obscene\n",
      "0.9875120702685147\n",
      "fit obscene\n",
      "0.9822573562703825\n",
      "fit obscene\n",
      "0.9795367148298655\n",
      "fit threat\n",
      "0.9873849541259485\n",
      "fit threat\n",
      "0.9885523557320977\n",
      "fit threat\n",
      "0.9894933963375029\n",
      "fit threat\n",
      "0.986381850589502\n",
      "fit threat\n",
      "0.9758446066720414\n",
      "fit insult\n",
      "0.9801475648682588\n",
      "fit insult\n",
      "0.9784588072111013\n",
      "fit insult\n",
      "0.9804492089994591\n",
      "fit insult\n",
      "0.9796015232396261\n",
      "fit insult\n",
      "0.9762497486472347\n",
      "fit identity_hate\n",
      "0.9797239821895679\n",
      "fit identity_hate\n",
      "0.9824519198657007\n",
      "fit identity_hate\n",
      "0.9815120226067415\n",
      "fit identity_hate\n",
      "0.9667270059370491\n",
      "fit identity_hate\n",
      "0.9791796939333319\n",
      "Saving out-of-fold\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3573e8c4513c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving out-of-fold\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msubmid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubmid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'oof_train_nblogreg.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, label in enumerate(class_names):\n",
    "    for train_idx, pred_idx in folds.split(train[label]):\n",
    "        print('fit', label)\n",
    "        m,r = get_mdl(train_features[train_idx], train[label][train_idx])\n",
    "        preds[:,i][pred_idx] = m.predict_proba(train_features[pred_idx].multiply(r))[:,1]\n",
    "        print(roc_auc_score(train[label][pred_idx], preds[:,i][pred_idx]))\n",
    "\n",
    "print(\"Saving out-of-fold\")\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = labels)], axis=1)\n",
    "submission.to_csv('oof_train_nblogreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving out-of-fold\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving out-of-fold\")\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = class_names)], axis=1)\n",
    "submission.to_csv('oof_train_nblogreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n",
      "Saving submission\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((len(test), len(class_names)))  \n",
    "\n",
    "for i, label in enumerate(class_names):\n",
    "    print('fit', label)\n",
    "    m,r = get_mdl(train_features, train[label])\n",
    "    preds[:,i] = m.predict_proba(test_features.multiply(r))[:,1]\n",
    "\n",
    "print(\"Saving submission\")\n",
    "final_submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "final_submission = pd.concat([final_submid, pd.DataFrame(preds, columns = class_names)], axis=1)\n",
    "final_submission.to_csv('nblogreg_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_funcs = []\n",
    "transforms = []\n",
    "logreg = LogisticRegression(solver='sag')\n",
    "logreg.name = \"Logistic regression sag\"\n",
    "gbm = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.9, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "gbm.name = \"LightGBM stacker\"\n",
    "models = [logreg]\n",
    "\n",
    "pipe = NlpPipeline(train, test, \"comment_text\", class_names, feature_funcs, transforms, models, word_index=None, pretrained=\"char n-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.train_features = train_features\n",
    "pipe.test_features = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating out-of-fold meta training set for stacker\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "toxic\n",
      "AUC: 0.9789751815516097\n",
      "AUC: 0.9774398964774006\n",
      "AUC: 0.9802334294472687\n",
      "AUC: 0.979530431066261\n",
      "AUC: 0.9795140199028408\n",
      "severe_toxic\n",
      "AUC: 0.9879613562810241\n",
      "AUC: 0.9895188664032896\n",
      "AUC: 0.9891020263293753\n",
      "AUC: 0.9894281553063295\n",
      "AUC: 0.9863817956924953\n",
      "obscene\n",
      "AUC: 0.991222385264418\n",
      "AUC: 0.9891188465187506\n",
      "AUC: 0.9918137457036066\n",
      "AUC: 0.9893193268028236\n",
      "AUC: 0.9905650626886647\n",
      "threat\n",
      "AUC: 0.9940803842063225\n",
      "AUC: 0.9905287984111222\n",
      "AUC: 0.992184448803875\n",
      "AUC: 0.9900140915396003\n",
      "AUC: 0.9849107378397103\n",
      "insult\n",
      "AUC: 0.9833657711944025\n",
      "AUC: 0.9831334299659442\n",
      "AUC: 0.9840607077470257\n",
      "AUC: 0.9827373877140632\n",
      "AUC: 0.9817399781701382\n",
      "identity_hate\n",
      "AUC: 0.9831047975307894\n",
      "AUC: 0.9877746879633794\n",
      "AUC: 0.98344247668577\n",
      "AUC: 0.9779700872739131\n",
      "AUC: 0.9862954755445621\n",
      "CV score: 0.9858489262008926\n",
      "Fitting and predicting\n",
      "Fitting submission classifier for toxic\n",
      "Fitting submission classifier for severe_toxic\n",
      "Fitting submission classifier for obscene\n",
      "Fitting submission classifier for threat\n",
      "Fitting submission classifier for insult\n",
      "Fitting submission classifier for identity_hate\n"
     ]
    }
   ],
   "source": [
    "pipe.fit_predict_oof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic regression sag': 0.9858489262008926}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "pipe.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

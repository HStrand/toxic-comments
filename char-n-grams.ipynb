{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv').fillna(' ')\n",
    "test = pd.read_csv('data\\\\test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=15000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.tocsr()\n",
    "test_features = test_features.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 35000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def get_mdl(x, y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.zeros((len(test), len(class_names)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.read_csv('data\\\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = np.zeros((len(train), len(class_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "0.962930469030776\n",
      "fit toxic\n",
      "0.9618735741865077\n",
      "fit toxic\n",
      "0.9639397137864535\n",
      "fit toxic\n",
      "0.9662076463464705\n",
      "fit toxic\n",
      "0.9632339798265157\n",
      "fit severe_toxic\n",
      "0.9822216726745506\n",
      "fit severe_toxic\n",
      "0.9849048617270015\n",
      "fit severe_toxic\n",
      "0.986091274828139\n",
      "fit severe_toxic\n",
      "0.9824602761740998\n",
      "fit severe_toxic\n",
      "0.9814096213444636\n",
      "fit obscene\n",
      "0.9854751993512636\n",
      "fit obscene\n",
      "0.9783850420512978\n",
      "fit obscene\n",
      "0.9875120702685147\n",
      "fit obscene\n",
      "0.9822573562703825\n",
      "fit obscene\n",
      "0.9795367148298655\n",
      "fit threat\n",
      "0.9873849541259485\n",
      "fit threat\n",
      "0.9885523557320977\n",
      "fit threat\n",
      "0.9894933963375029\n",
      "fit threat\n",
      "0.986381850589502\n",
      "fit threat\n",
      "0.9758446066720414\n",
      "fit insult\n",
      "0.9801475648682588\n",
      "fit insult\n",
      "0.9784588072111013\n",
      "fit insult\n",
      "0.9804492089994591\n",
      "fit insult\n",
      "0.9796015232396261\n",
      "fit insult\n",
      "0.9762497486472347\n",
      "fit identity_hate\n",
      "0.9797239821895679\n",
      "fit identity_hate\n",
      "0.9824519198657007\n",
      "fit identity_hate\n",
      "0.9815120226067415\n",
      "fit identity_hate\n",
      "0.9667270059370491\n",
      "fit identity_hate\n",
      "0.9791796939333319\n",
      "Saving out-of-fold\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3573e8c4513c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving out-of-fold\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msubmid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubmid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'oof_train_nblogreg.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, label in enumerate(class_names):\n",
    "    for train_idx, pred_idx in folds.split(train[label]):\n",
    "        print('fit', label)\n",
    "        m,r = get_mdl(train_features[train_idx], train[label][train_idx])\n",
    "        preds[:,i][pred_idx] = m.predict_proba(train_features[pred_idx].multiply(r))[:,1]\n",
    "        print(roc_auc_score(train[label][pred_idx], preds[:,i][pred_idx]))\n",
    "\n",
    "print(\"Saving out-of-fold\")\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = labels)], axis=1)\n",
    "submission.to_csv('oof_train_nblogreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving out-of-fold\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving out-of-fold\")\n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = class_names)], axis=1)\n",
    "submission.to_csv('oof_train_nblogreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n",
      "Saving submission\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((len(test), len(class_names)))  \n",
    "\n",
    "for i, label in enumerate(class_names):\n",
    "    print('fit', label)\n",
    "    m,r = get_mdl(train_features, train[label])\n",
    "    preds[:,i] = m.predict_proba(test_features.multiply(r))[:,1]\n",
    "\n",
    "print(\"Saving submission\")\n",
    "final_submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "final_submission = pd.concat([final_submid, pd.DataFrame(preds, columns = class_names)], axis=1)\n",
    "final_submission.to_csv('nblogreg_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp_pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_funcs = []\n",
    "transforms = []\n",
    "logreg = LogisticRegression(solver='sag')\n",
    "logreg.name = \"Logistic regression sag\"\n",
    "gbm = lgb.LGBMClassifier(metric=\"auc\", num_leaves=31, boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.9, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.5)\n",
    "gbm.name = \"LightGBM\"\n",
    "models = [gbm]\n",
    "\n",
    "pipe = NlpPipeline(train, test, \"comment_text\", class_names, feature_funcs, transforms, models, word_index=None, pretrained=\"char n-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.train_features = train_features.tocsr()\n",
    "pipe.test_features = test_features.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating out-of-fold meta training set for stacker\n",
      "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',\n",
      "        class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,\n",
      "        learning_rate=0.1, max_depth=-1, metric='auc',\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.5, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=1)\n",
      "toxic\n",
      "AUC: 0.9728746932413219\n",
      "AUC: 0.9723599538024149\n",
      "AUC: 0.9732761099152616\n",
      "AUC: 0.972855369272012\n",
      "AUC: 0.9752151933063815\n",
      "severe_toxic\n",
      "AUC: 0.9842319423795322\n",
      "AUC: 0.9863290131419818\n",
      "AUC: 0.9873782156006304\n",
      "AUC: 0.9880083161805161\n",
      "AUC: 0.9866109078900575\n",
      "obscene\n",
      "AUC: 0.9921863668835557\n",
      "AUC: 0.9905143323349049\n",
      "AUC: 0.9928259553567866\n",
      "AUC: 0.9902177101855681\n",
      "AUC: 0.9913056833285869\n",
      "threat\n",
      "AUC: 0.9929514640736021\n",
      "AUC: 0.9813570285777337\n",
      "AUC: 0.9835819210171094\n",
      "AUC: 0.9828555635011952\n",
      "AUC: 0.9812100748761721\n",
      "insult\n",
      "AUC: 0.9800973049952713\n",
      "AUC: 0.9808431400840724\n",
      "AUC: 0.9810932012758294\n",
      "AUC: 0.9796499212942537\n",
      "AUC: 0.981234309366665\n",
      "identity_hate\n",
      "AUC: 0.9782683922055586\n",
      "AUC: 0.9861194355510234\n",
      "AUC: 0.9733138482738043\n",
      "AUC: 0.9754837828276025\n",
      "AUC: 0.9831112029556229\n",
      "CV score: 0.982578678456501\n",
      "Fitting and predicting\n",
      "Fitting submission classifier for toxic\n",
      "Fitting submission classifier for severe_toxic\n",
      "Fitting submission classifier for obscene\n",
      "Fitting submission classifier for threat\n",
      "Fitting submission classifier for insult\n",
      "Fitting submission classifier for identity_hate\n"
     ]
    }
   ],
   "source": [
    "pipe.fit_predict_oof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submissions\n"
     ]
    }
   ],
   "source": [
    "pipe.create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
